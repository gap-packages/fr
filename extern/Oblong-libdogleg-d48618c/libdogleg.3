.\" Automatically generated by Pod::Man 2.23 (Pod::Simple 3.14)
.\"
.\" Standard preamble:
.\" ========================================================================
.de Sp \" Vertical space (when we can't use .PP)
.if t .sp .5v
.if n .sp
..
.de Vb \" Begin verbatim text
.ft CW
.nf
.ne \\$1
..
.de Ve \" End verbatim text
.ft R
.fi
..
.\" Set up some character translations and predefined strings.  \*(-- will
.\" give an unbreakable dash, \*(PI will give pi, \*(L" will give a left
.\" double quote, and \*(R" will give a right double quote.  \*(C+ will
.\" give a nicer C++.  Capital omega is used to do unbreakable dashes and
.\" therefore won't be available.  \*(C` and \*(C' expand to `' in nroff,
.\" nothing in troff, for use with C<>.
.tr \(*W-
.ds C+ C\v'-.1v'\h'-1p'\s-2+\h'-1p'+\s0\v'.1v'\h'-1p'
.ie n \{\
.    ds -- \(*W-
.    ds PI pi
.    if (\n(.H=4u)&(1m=24u) .ds -- \(*W\h'-12u'\(*W\h'-12u'-\" diablo 10 pitch
.    if (\n(.H=4u)&(1m=20u) .ds -- \(*W\h'-12u'\(*W\h'-8u'-\"  diablo 12 pitch
.    ds L" ""
.    ds R" ""
.    ds C` ""
.    ds C' ""
'br\}
.el\{\
.    ds -- \|\(em\|
.    ds PI \(*p
.    ds L" ``
.    ds R" ''
'br\}
.\"
.\" Escape single quotes in literal strings from groff's Unicode transform.
.ie \n(.g .ds Aq \(aq
.el       .ds Aq '
.\"
.\" If the F register is turned on, we'll generate index entries on stderr for
.\" titles (.TH), headers (.SH), subsections (.SS), items (.Ip), and index
.\" entries marked with X<> in POD.  Of course, you'll have to process the
.\" output yourself in some meaningful fashion.
.ie \nF \{\
.    de IX
.    tm Index:\\$1\t\\n%\t"\\$2"
..
.    nr % 0
.    rr F
.\}
.el \{\
.    de IX
..
.\}
.\"
.\" Accent mark definitions (@(#)ms.acc 1.5 88/02/08 SMI; from UCB 4.2).
.\" Fear.  Run.  Save yourself.  No user-serviceable parts.
.    \" fudge factors for nroff and troff
.if n \{\
.    ds #H 0
.    ds #V .8m
.    ds #F .3m
.    ds #[ \f1
.    ds #] \fP
.\}
.if t \{\
.    ds #H ((1u-(\\\\n(.fu%2u))*.13m)
.    ds #V .6m
.    ds #F 0
.    ds #[ \&
.    ds #] \&
.\}
.    \" simple accents for nroff and troff
.if n \{\
.    ds ' \&
.    ds ` \&
.    ds ^ \&
.    ds , \&
.    ds ~ ~
.    ds /
.\}
.if t \{\
.    ds ' \\k:\h'-(\\n(.wu*8/10-\*(#H)'\'\h"|\\n:u"
.    ds ` \\k:\h'-(\\n(.wu*8/10-\*(#H)'\`\h'|\\n:u'
.    ds ^ \\k:\h'-(\\n(.wu*10/11-\*(#H)'^\h'|\\n:u'
.    ds , \\k:\h'-(\\n(.wu*8/10)',\h'|\\n:u'
.    ds ~ \\k:\h'-(\\n(.wu-\*(#H-.1m)'~\h'|\\n:u'
.    ds / \\k:\h'-(\\n(.wu*8/10-\*(#H)'\z\(sl\h'|\\n:u'
.\}
.    \" troff and (daisy-wheel) nroff accents
.ds : \\k:\h'-(\\n(.wu*8/10-\*(#H+.1m+\*(#F)'\v'-\*(#V'\z.\h'.2m+\*(#F'.\h'|\\n:u'\v'\*(#V'
.ds 8 \h'\*(#H'\(*b\h'-\*(#H'
.ds o \\k:\h'-(\\n(.wu+\w'\(de'u-\*(#H)/2u'\v'-.3n'\*(#[\z\(de\v'.3n'\h'|\\n:u'\*(#]
.ds d- \h'\*(#H'\(pd\h'-\w'~'u'\v'-.25m'\f2\(hy\fP\v'.25m'\h'-\*(#H'
.ds D- D\\k:\h'-\w'D'u'\v'-.11m'\z\(hy\v'.11m'\h'|\\n:u'
.ds th \*(#[\v'.3m'\s+1I\s-1\v'-.3m'\h'-(\w'I'u*2/3)'\s-1o\s+1\*(#]
.ds Th \*(#[\s+2I\s-2\h'-\w'I'u*3/5'\v'-.3m'o\v'.3m'\*(#]
.ds ae a\h'-(\w'a'u*4/10)'e
.ds Ae A\h'-(\w'A'u*4/10)'E
.    \" corrections for vroff
.if v .ds ~ \\k:\h'-(\\n(.wu*9/10-\*(#H)'\s-2\u~\d\s+2\h'|\\n:u'
.if v .ds ^ \\k:\h'-(\\n(.wu*10/11-\*(#H)'\v'-.4m'^\v'.4m'\h'|\\n:u'
.    \" for low resolution devices (crt and lpr)
.if \n(.H>23 .if \n(.V>19 \
\{\
.    ds : e
.    ds 8 ss
.    ds o a
.    ds d- d\h'-1'\(ga
.    ds D- D\h'-1'\(hy
.    ds th \o'bp'
.    ds Th \o'LP'
.    ds ae ae
.    ds Ae AE
.\}
.rm #[ #] #H #V #F C
.\" ========================================================================
.\"
.IX Title "LIBDOGLEG 3"
.TH LIBDOGLEG 3 "2012-10-03" "libdogleg 0.7" "libdogleg: Powell's dogleg method"
.\" For nroff, turn off justification.  Always turn off hyphenation; it makes
.\" way too many mistakes in technical documents.
.if n .ad l
.nh
.SH "NAME"
libdogleg \- A general purpose sparse optimizer to solve data fitting problems,
such as sparse bundle adjustment.
.SH "DESCRIPTION"
.IX Header "DESCRIPTION"
This is a library for solving large-scale nonlinear optimization problems. By employing sparse
linear algebra, it is taylored for problems that have weak coupling between the optimization
variables. For appropriately sparse problems this results in \fImassive\fR performance gains.
.PP
The main task of this library is to find the vector \fBp\fR that minimizes
.PP
norm2( \fBx\fR )
.PP
where \fBx\fR = \fIf\fR(\fBp\fR) is a vector that has higher dimensionality than \fBp\fR. The user passes in a
callback function (of type \f(CW\*(C`dogleg_callback_t\*(C'\fR) that takes in the vector \fBp\fR and returns the
vector \fBx\fR and a matrix of derivatives \fBJ\fR = d\fBf\fR/d\fBp\fR. \fBJ\fR is a matrix with a row for each element
of \fIf\fR and a column for each element of \fBp\fR. \fBJ\fR is a sparse matrix, which results in substantial
increases in computational efficiency if most entries of \fBJ\fR are 0. \fBJ\fR is stored row-first in the
callback routine. libdogleg uses a column-first data representation so it references the transpose
of \fBJ\fR (called \fBJt\fR). \fBJ\fR stored row-first is identical to \fBJt\fR stored column-first; this is purely a
naming choice.
.PP
This library implements Powell's dog-leg algorithm to solve the problem. Like the more-widely-known
Levenberg-Marquardt algorithm, Powell's dog-leg algorithm solves a nonlinear optimization problem by
interpolating between a Gauss-Newton step and a gradient descent step. Improvements over \s-1LM\s0 are
.IP "\(bu" 4
a more natural representation of the linearity of the operating point (trust region size vs
a vague lambda term).
.IP "\(bu" 4
significant efficiency gains, since a matrix inversion isn't needed to retry a rejected step
.PP
The algorithm is described in many places, originally in
.PP
M. Powell. A Hybrid Method for Nonlinear Equations. In P. Rabinowitz, editor, Numerical Methods for
Nonlinear Algebraic Equations, pages 87\-144.  Gordon and Breach Science, London, 1970.
.PP
Various enhancements to Powell's original method are described in the literature; at this time only
the original algorithm is implemented here.
.PP
The sparse matrix algebra is handled by the \s-1CHOLMOD\s0 library, written by Tim Davis. Parts of \s-1CHOLMOD\s0
are licensed under the \s-1GPL\s0 and parts under the \s-1LGPL\s0. Only the \s-1LGPL\s0 pieces are used here, allowing
libdogleg to be licensed under the \s-1LGPL\s0 as well. Due to this I lose some convenience (all simple
sparse matrix arithmetic in \s-1CHOLMOD\s0 is GPL-ed) and some performance (the fancier computational
methods, such as supernodal analysis are GPL-ed). For my current applications the performance losses
are minor.
.SH "FUNCTIONS AND TYPES"
.IX Header "FUNCTIONS AND TYPES"
.SS "Main \s-1API\s0"
.IX Subsection "Main API"
\fIdogleg_optimize\fR
.IX Subsection "dogleg_optimize"
.PP
This is the main call to the library. Its declared as
.PP
.Vb 4
\& double dogleg_optimize(double* p, unsigned int Nstate,
\&                        unsigned int Nmeas, unsigned int NJnnz,
\&                        dogleg_callback_t* f, void* cookie,
\&                        dogleg_solverContext_t** returnContext);
.Ve
.IP "\(bu" 4
\&\fBp\fR is the initial estimate of the state vector (and holds the final result)
.IP "\(bu" 4
\&\f(CW\*(C`Nstate\*(C'\fR specifies the number of optimization variables (elements of \fBp\fR)
.IP "\(bu" 4
\&\f(CW\*(C`Nmeas\*(C'\fR specifies the number of measurements (elements of \fBx\fR). \f(CW\*(C`Nmeas >= Nstate\*(C'\fR is a
requirement
.IP "\(bu" 4
\&\f(CW\*(C`NJnnz\*(C'\fR specifies the number of non-zero elements of the jacobian matrix d\fBf\fR/d\fBp\fR. In a
dense matrix \f(CW\*(C`Jnnz = Nstate*Nmeas\*(C'\fR. We are dealing with sparse jacobians, so \f(CW\*(C`NJnnz\*(C'\fR should be far
less. If not, libdogleg is not an appropriate routine to solve this problem.
.IP "\(bu" 4
\&\f(CW\*(C`f\*(C'\fR specifies the callback function that the optimization routine calls to sample the problem
being solved
.IP "\(bu" 4
\&\f(CW\*(C`cookie\*(C'\fR is an arbitrary data pointer passed untouched to \f(CW\*(C`f\*(C'\fR
.IP "\(bu" 4
If not \f(CW\*(C`NULL\*(C'\fR, \f(CW\*(C`returnContext\*(C'\fR can be used to retrieve the full
context structure from the solver. This can be useful since this structure
contains the latest operating point values. It also has an active
\&\f(CW\*(C`cholmod_common\*(C'\fR structure, which can be reused if more \s-1CHOLMOD\s0 routines need
to be called externally. \fIIf this data is requested, the user is required to
free it with \f(CI\*(C`dogleg_freeContext\*(C'\fI when done\fR.
.PP
\&\f(CW\*(C`dogleg_optimize\*(C'\fR returns norm2( \fBx\fR ) at the minimum, or a negative value if an error occurred.
.PP
\fIdogleg_freeContext\fR
.IX Subsection "dogleg_freeContext"
.PP
Used to deallocate memory used for an optimization cycle. Defined as:
.PP
.Vb 1
\& void dogleg_freeContext(dogleg_solverContext_t** ctx);
.Ve
.PP
If a pointer to a context is not requested (by passing \f(CW\*(C`returnContext = NULL\*(C'\fR
to \f(CW\*(C`dogleg_optimize\*(C'\fR), libdogleg calls this routine automatically. If the user
\&\fIdid\fR retrieve this pointer, though, it must be freed with
\&\f(CW\*(C`dogleg_freeContext\*(C'\fR when the user is finished.
.PP
\fIdogleg_testGradient\fR
.IX Subsection "dogleg_testGradient"
.PP
libdogleg requires the user to compute the jacobian matrix \fBJ\fR. This is a performance optimization,
since \fBJ\fR could be computed by differences of \fBx\fR. This optimization is often worth the extra
effort, but it creates a possibility that \fBJ\fR will have a mistake and \fBJ\fR = d\fBf\fR/d\fBp\fR would not
be true. To find these types of issues, the user can call
.PP
.Vb 3
\& void dogleg_testGradient(unsigned int var, const double* p0,
\&                          unsigned int Nstate, unsigned int Nmeas, unsigned int NJnnz,
\&                          dogleg_callback_t* f, void* cookie);
.Ve
.PP
This function computes the jacobian with center differences and compares the result with the
jacobian computed by the callback function. It is recommended to do this for every variable while
developing the program that uses libdogleg.
.IP "\(bu" 4
\&\f(CW\*(C`var\*(C'\fR is the index of the variable being tested
.IP "\(bu" 4
\&\f(CW\*(C`p0\*(C'\fR is the state vector \fBp\fR where we're evaluating the jacobian
.IP "\(bu" 4
\&\f(CW\*(C`Nstate\*(C'\fR, \f(CW\*(C`Nmeas\*(C'\fR, \f(CW\*(C`NJnnz\*(C'\fR are the number of state variables, measurements and non-zero jacobian elements, as before
.IP "\(bu" 4
\&\f(CW\*(C`f\*(C'\fR is the callback function, as before
.IP "\(bu" 4
\&\f(CW\*(C`cookie\*(C'\fR is the user data, as before
.PP
This function returns nothing, but prints out the test results.
.PP
\fIdogleg_callback_t\fR
.IX Subsection "dogleg_callback_t"
.PP
The main user callback that specifies the optimization problem has type
.PP
.Vb 4
\& typedef void (dogleg_callback_t)(const double*   p,
\&                                  double*         x,
\&                                  cholmod_sparse* Jt,
\&                                  void*           cookie);
.Ve
.IP "\(bu" 4
\&\fBp\fR is the current state vector
.IP "\(bu" 4
\&\fBx\fR is the resulting \fIf\fR(\fBp\fR)
.IP "\(bu" 4
\&\fBJt\fR is the transpose of d\fBf\fR/d\fBp\fR at \fBp\fR. As mentioned previously, \fBJt\fR is stored
column-first by \s-1CHOLMOD\s0, which can be interpreted as storing \fBJ\fR row-first by the user-defined
callback routine
.IP "\(bu" 4
The \f(CW\*(C`cookie\*(C'\fR is the user-defined arbitrary data passed into \f(CW\*(C`dogleg_optimize\*(C'\fR.
.PP
\fIdogleg_solverContext_t\fR
.IX Subsection "dogleg_solverContext_t"
.PP
This is the solver context that can be retrieved through the \f(CW\*(C`returnContext\*(C'\fR
parameter of the \f(CW\*(C`dogleg_optimize\*(C'\fR call. This structure contains \fIall\fR the
internal state used by the solver. If requested, the user is responsible for
calling \f(CW\*(C`dogleg_freeContext\*(C'\fR when done. This structure is defined as:
.PP
.Vb 3
\& typedef struct
\& {
\&   cholmod_common  common;
\& 
\&   dogleg_callback_t* f;
\&   void*              cookie;
\& 
\&   // between steps, beforeStep contains the operating point of the last step.
\&   // afterStep is ONLY used while making the step. Externally, use beforeStep
\&   // unless you really know what you\*(Aqre doing
\&   dogleg_operatingPoint_t* beforeStep;
\&   dogleg_operatingPoint_t* afterStep;
\&
\&   // The result of the last JtJ factorization performed. Note that JtJ is not
\&   // necessarily factorized at every step, so this is NOT guaranteed to contain
\&   // the factorization of the most recent JtJ
\&   cholmod_factor*          factorization;
\& 
\&   // Have I ever seen a singular JtJ? If so, I add a small constant to the
\&   // diagonal from that point on. This is a simple and fast way to deal with
\&   // singularities. This is suboptimal but works for me for now.
\&   int               wasPositiveSemidefinite;
\& } dogleg_solverContext_t;
.Ve
.PP
Some of the members are copies of the data passed into \f(CW\*(C`dogleg_optimize\*(C'\fR; some
others are internal state. Of potential interest are
.IP "\(bu" 4
\&\f(CW\*(C`common\*(C'\fR is a cholmod_common structure used by all \s-1CHOLMOD\s0 calls. This
can be used for any extra \s-1CHOLMOD\s0 work the user may want to do
.IP "\(bu" 4
\&\f(CW\*(C`beforeStep\*(C'\fR contains the operating point of the optimum solution. The
user can analyze this data without the need to re-call the callback routine.
.PP
\fIdogleg_operatingPoint_t\fR
.IX Subsection "dogleg_operatingPoint_t"
.PP
An operating point of the solver. This is a part of \f(CW\*(C`dogleg_solverContext_t\*(C'\fR.
Various variables describing the operating point such as \fBp\fR, \fBJ\fR, \fBx\fR,
\&\fBnorm2(x)\fR and \fBJt x\fR are available. All of the just-mentioned variables are
computed at every step and are thus always valid.
.PP
.Vb 8
\& // an operating point of the solver
\& typedef struct
\& {
\&   double*         p;
\&   double*         x;
\&   double          norm2_x;
\&   cholmod_sparse* Jt;
\&   double*         Jt_x;
\& 
\&   // the cached update vectors. It\*(Aqs useful to cache these so that when a step is rejected, we can
\&   // reuse these when we retry
\&   double*        updateCauchy;
\&   cholmod_dense* updateGN_cholmoddense;
\&   double         updateCauchy_lensq, updateGN_lensq; // update vector lengths
\& 
\&   // whether the current update vectors are correct or not
\&   int updateCauchy_valid, updateGN_valid;
\& 
\&   int didStepToEdgeOfTrustRegion;
\& } dogleg_operatingPoint_t;
.Ve
.SS "Parameters"
.IX Subsection "Parameters"
It is not required to call any of these, but it's highly recommended to set the initial trust-region
size and the termination thresholds to match the problem being solved. Furthermore, it's highly
recommended for the problem being solved to be scaled so that every state variable affects the
objective norm2( \fBx\fR ) equally. This makes this method's concept of \*(L"trust region\*(R" much more
well-defined and makes the termination criteria work correctly.
.PP
\fIdogleg_setMaxIterations\fR
.IX Subsection "dogleg_setMaxIterations"
.PP
To set the maximum number of solver iterations, call
.PP
.Vb 1
\& void dogleg_setMaxIterations(int n);
.Ve
.PP
\fIdogleg_setDebug\fR
.IX Subsection "dogleg_setDebug"
.PP
To turn on debug output, call
.PP
.Vb 1
\& void dogleg_setDebug(int debug);
.Ve
.PP
with a non-zero value for \f(CW\*(C`debug\*(C'\fR. By default, debug output is disabled.
.PP
\fIdogleg_setInitialTrustregion\fR
.IX Subsection "dogleg_setInitialTrustregion"
.PP
The optimization method keeps track of a trust region size. Here, the trust region is a ball in
R^Nstate. When the method takes a step \fBp\fR \-> \fBp + delta_p\fR, it makes sure that
.PP
sqrt(\ norm2(\ \fBdelta_p\fR\ )\ )\ <\ trust\ region\ size.
.PP
The initial value of the trust region size can be set with
.PP
.Vb 1
\& void dogleg_setInitialTrustregion(double t);
.Ve
.PP
The dogleg algorithm is efficient when recomputing a rejected step for a smaller trust region, so
set the initial trust region size to a value larger to a reasonable estimate; the method will
quickly shrink the trust region to the correct size.
.PP
\fIdogleg_setThresholds\fR
.IX Subsection "dogleg_setThresholds"
.PP
The routine exits when the maximum number of iterations is exceeded, or a termination threshold is
hit, whichever happens first. The termination thresholds are all designed to trigger when very slow
progress is being made. If all went well, this slow progress is due to us finding the optimum. There
are 3 termination thresholds:
.IP "\(bu" 4
The function being minimized is E = norm2( \fBx\fR ) where \fBx\fR = \fIf\fR(\fBp\fR).
.Sp
dE/d\fBp\fR = 2*\fBJt\fR*\fBx\fR where \fBJt\fR is transpose(d\fBx\fR/d\fBp\fR).
.Sp
.Vb 2
\& if( for every i  fabs(Jt_x[i]) < JT_X_THRESHOLD )
\& { we are done; }
.Ve
.IP "\(bu" 4
The method takes discrete steps: \fBp\fR \-> \fBp + delta_p\fR
.Sp
.Vb 2
\& if( for every i  fabs(delta_p[i]) < UPDATE_THRESHOLD)
\& { we are done; }
.Ve
.IP "\(bu" 4
The method dynamically controls the trust region.
.Sp
.Vb 2
\& if(trustregion < TRUSTREGION_THRESHOLD)
\& { we are done; }
.Ve
.PP
To set these threholds, call
.PP
.Vb 1
\& void dogleg_setThresholds(double Jt_x, double update, double trustregion);
.Ve
.PP
To leave a particular threshold alone, specify a negative value.
.PP
\fIdogleg_setTrustregionUpdateParameters\fR
.IX Subsection "dogleg_setTrustregionUpdateParameters"
.PP
This function sets the parameters that control when and how the trust region is updated. The default
values should work well in most cases, and shouldn't need to be tweaked.
.PP
Declaration looks like
.PP
.Vb 2
\& void dogleg_setTrustregionUpdateParameters(double downFactor, double downThreshold,
\&                                            double upFactor,   double upThreshold);
.Ve
.PP
To see what the parameters do, look at \f(CW\*(C`evaluateStep_adjustTrustRegion\*(C'\fR in the source. Again, these
should just work as is.
.SH "BUGS"
.IX Header "BUGS"
The current implementation doesn't handle a singular \fBJtJ\fR gracefully (\fBJtJ\fR =
\&\fBJt\fR * \fBJ\fR). Analytically, \fBJtJ\fR is at worst positive semi-definite (has 0 eigenvalues). If a
singular \fBJtJ\fR is ever encountered, from that point on, \fBJtJ\fR + lambda*\fBI\fR is inverted instead
for some positive constant lambda. This makes the matrix strictly positive definite, but is
sloppy. At least I should vary lambda. In my current applications, a singular \fBJtJ\fR only occurs if
at a particular operating point the vector \fBx\fR has no dependence at all on some elements of
\&\fBp\fR. In the general case other causes could exist, though.
.PP
There's an inefficiency in that the callback always returns \fBx\fR and \fBJ\fR. When I evaluate and
reject a step, I do not end up using \fBJ\fR at all. Dependng on the callback function, it may be
better to ask for \fBx\fR and then, if the step is accepted, to ask for \fBJ\fR.
.SH "AUTHOR"
.IX Header "AUTHOR"
Dima Kogan, \f(CW\*(C`<dima@secretsauce.net>\*(C'\fR
.SH "LICENSE AND COPYRIGHT"
.IX Header "LICENSE AND COPYRIGHT"
Copyright 2011 Oblong Industries
.PP
This program is free software: you can redistribute it and/or modify it under the terms of the \s-1GNU\s0
Lesser General Public License as published by the Free Software Foundation, either version 3 of the
License, or (at your option) any later version.
.PP
This program is distributed in the hope that it will be useful, but \s-1WITHOUT\s0 \s-1ANY\s0 \s-1WARRANTY\s0; without
even the implied warranty of \s-1MERCHANTABILITY\s0 or \s-1FITNESS\s0 \s-1FOR\s0 A \s-1PARTICULAR\s0 \s-1PURPOSE\s0.  See the \s-1GNU\s0
Lesser General Public License for more details.
.PP
The full text of the license is available at <http://www.gnu.org/licenses>
